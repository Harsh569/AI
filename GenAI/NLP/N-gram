An N-gram is a contiguous sequence of N items (usually words or characters) from a given text.
üî¢ Types of N-grams:

Unigram (1-gram): Single words ‚Üí ["Harshad", "is", "learning"]
Bigram (2-gram): Pairs of words ‚Üí [("Harshad", "is"), ("is", "learning")]
Trigram (3-gram): Triplets ‚Üí [("Harshad", "is", "learning")]


import nltk
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

# Download tokenizer
nltk.download('punkt')

# Sample text
text = "Harshad is learning Natural Language Processing"

# Tokenize into words
tokens = word_tokenize(text)

# Generate bigrams
bigrams = list(ngrams(tokens, 2))
print("Bigrams:", bigrams)

# Generate trigrams
trigrams = list(ngrams(tokens, 3))
print("Trigrams:", trigrams)

Output:  ('is', 'learning'), ('learning', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing')]
Trigrams: [('Harshad', 'is', 'learning'), ('is', 'learning', 'Natural'), ('learning', 'Natural', 'Language'), ('Natural', 'Language', 'Processing')]

üéØ Purpose of N-gram Tokenization
N-gram tokenization helps capture contextual relationships between words or characters in a sequence. Instead of treating each word independently (like in unigram models), N-grams consider sequences of N items, which improves understanding of language structure.

‚úÖ Key Purposes & Applications
1. Language Modeling

Predict the next word based on previous N-1 words.
Example: In bigram model, "I am" ‚Üí likely next word is "happy".

2. Text Classification

Spam detection, sentiment analysis, etc.
N-grams help capture phrases like ‚Äúnot good‚Äù or ‚Äúvery bad‚Äù.

3. Information Retrieval

Improves search relevance by matching phrases instead of single words.

4. Machine Translation

Helps align phrases between source and target languages.

5. Speech Recognition

Predicts word sequences based on phonetic patterns.

6. Plagiarism Detection

Matches sequences of words across documents.

